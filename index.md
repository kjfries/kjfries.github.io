# Bio

Kevin's work seeks to find unique solutions to society's grand challenges through interdisciplinary research and design. Trained as a civil engineer but with an extensive background in systems engineering and machine learning, Kevin has leveraged his unique expertise to tackle problems in natural resource management, disaster response, hydrometeorology, and economic development.

## Education
**_University of Michigan_**
- PhD - Civil Engineering
- MS - Electrical and Computer Engineering - Machinge Learning
- MS - Infrastructure Systems Engineering

**_University of Maryland_**
- BS - Civil and Environmental Engineering, _summa cum laude_

## Experience
**_Ceres Imaging_**
- Geospatial Data Scientist
- Feb 2020 - present

**_DataKind SF_**
- Data Ambassador
- Jan 2021 - present

**_One Concern_**
- Data Scientist - Flood
- Apr 2018 - Feb 2020

## Skills
- Geopandas/rasterio
- pandas/numpy
- scikit-image/scikit-learn
- Tensorflow/Keras
- Containerization (Docker)

## Publications
[**Big Ship Data: Using vessel measurements to improve estimates of temperature and wind speed on the Great Lakes**](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/2016WR020084)

Abstract: The sheer size of many water systems challenges the ability of in situ sensor networks to resolve spatiotemporal variability of hydrologic processes. New sources of vastly distributed and mobile measurements are, however, emerging to potentially fill these observational gaps. This paper poses the question: How can nontraditional measurements, such as those made by volunteer ship captains, be used to improve hydrometeorological estimates across large surface water systems? We answer this question through the analysis of one of the largest such data sets: an unprecedented collection of one million unique measurements made by ships on the North American Great Lakes from 2006 to 2014. We introduce a flexible probabilistic framework, which can be used to integrate ship measurements, or other sets of irregular point measurements, into contiguous data sets. The performance of this framework is validated through the development of a new ship‐based spatial data product of water temperature, air temperature, and wind speed across the Great Lakes. An analysis of the final data product suggests that the availability of measurements across the Great Lakes will continue to play a large role in the confidence with which these large surface water systems can be studied and modeled. We discuss how this general and flexible approach can be applied to similar data sets, and how it will be of use to those seeking to merge large collections of measurements with other sources of data, such as physical models or remotely sensed products.

[**Using Sensor Data to Dynamically Map Large‐Scale Models to Site‐Scale Forecasts: A Case Study Using the National Water Model**](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2017WR022498)

Abstract: There has been an explosive growth in the ability to model large water systems. While these models are effective at routing water across massive scales, they do not yet forecast the street‐level information desired by local decision makers. Simultaneously, the increasing affordability of sensors has made it possible for even small communities to measure the state of their watersheds. However, these real‐time measurements are often not attached to a predictive model, thus making them less useful for applications like flood warnings. In this paper, we ask the question: how can highly localized forecasts be generated by fusing site‐scale sensor measurements with outputs from large‐scale models? Rather than altering the larger physical model, our approach uses the outputs of the unmodified model as the inputs to a dynamical system. To evaluate the approach, a case study is carried out across the U.S. state of Iowa using publicly available measurements from over 180 water level sensors and outputs from the National Water Model. The approach performs well across a third of the studied sites, as quantified by a high normalized root mean squared error. A performance classification is carried out based on Principal Component Analysis and Random Forests. We discuss how these results will enable stakeholders with local measurements to quickly benefit from large‐scale models without needing to run or modify the models themselves. The results are also placed into a broader sensor‐placement context to provide guidance on how investments into local measurements can be made to maximize predictive benefits.